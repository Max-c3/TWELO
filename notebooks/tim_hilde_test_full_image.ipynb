{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    path_images = \"./drive/MyDrive/obj_Train_data\"\n",
        "    path_annot = \"./drive/MyDrive/obj_Train_data\"\n",
        "else:\n",
        "    from kestrix.data import download_raw_data\n",
        "    download_raw_data()\n",
        "    path_images = \"./data/kestrix/raw\"\n",
        "    path_annot = \"./data/kestrix/raw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQAl1upmwvny",
        "outputId": "d2a45083-313b-4918-b63f-f3b7b6826c82"
      },
      "outputs": [],
      "source": [
        "#!pip install keras-cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h3F7eZywocm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm # show progress bars\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_cv\n",
        "\n",
        "from keras_cv import bounding_box\n",
        "from keras_cv import visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf0I0xtcwoco"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "SPLIT_RATIO = 0.2\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCH = 5\n",
        "GLOBAL_CLIPNORM = 10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34EcgLshwoco"
      },
      "outputs": [],
      "source": [
        "bounding_box_format = \"center_xywh\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNlvdxn0woco"
      },
      "outputs": [],
      "source": [
        "# Creating a dictionary for the classes\n",
        "class_ids = [\n",
        "    'car',\n",
        "    'person',\n",
        "]\n",
        "\n",
        "class_mapping = dict(zip(range(len(class_ids)),class_ids))\n",
        "class_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZtoio8wwocp"
      },
      "outputs": [],
      "source": [
        "txt_files = sorted(\n",
        "    [\n",
        "        os.path.join(path_annot, file_name)\n",
        "        for file_name in os.listdir(path_annot)\n",
        "        if file_name.endswith(\".txt\")\n",
        "    ]\n",
        ")\n",
        "txt_files[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXDtaguQwocp"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(txt_file):\n",
        "    with open(txt_file) as file:\n",
        "        lines = file.readlines()\n",
        "        file_name = Path(file.name).stem\n",
        "\n",
        "    image_path = os.path.join(path_images, file_name + \".JPG\")\n",
        "    boxes = []\n",
        "    class_ids = []\n",
        "    for line in lines:\n",
        "        line = line.split()\n",
        "\n",
        "        cls = float(line[0])\n",
        "        class_ids.append(cls)\n",
        "\n",
        "        x = float(line[1]) * 4000\n",
        "        y = float(line[2]) * 3000\n",
        "        width = float(line[3]) *4000\n",
        "        height = float(line[4]) * 3000\n",
        "\n",
        "        boxes.append([x, y, width, height])\n",
        "\n",
        "    return image_path, boxes, class_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weBit1rIwocp"
      },
      "outputs": [],
      "source": [
        "image_paths = []\n",
        "bbox = []\n",
        "classes = []\n",
        "for txt_file in txt_files:\n",
        "    image_path, boxes, class_ids = parse_annotation(txt_file)\n",
        "    image_paths.append(image_path)\n",
        "    bbox.append(boxes)\n",
        "    classes.append(class_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkrF6e9Owocp"
      },
      "outputs": [],
      "source": [
        "classes[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVzeL2luwocq"
      },
      "outputs": [],
      "source": [
        "bbox[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1jjNoF1wocq"
      },
      "outputs": [],
      "source": [
        "# creating ragged tensors because the number of objects varies\n",
        "# from image to image\n",
        "bbox = tf.ragged.constant(bbox)\n",
        "classes = tf.ragged.constant(classes)\n",
        "image_paths = tf.ragged.constant(image_paths)\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDh9K8t6wocr"
      },
      "outputs": [],
      "source": [
        "# Splitting data\n",
        "# Determine number of validation data\n",
        "num_val = int(len(txt_files) * SPLIT_RATIO)\n",
        "\n",
        "# split into train and validation\n",
        "# TODO change into random split via train_test_split\n",
        "val_data = data.take(num_val)\n",
        "train_data = data.skip(num_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbPb8ze8wocr"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_dataset(image_path, classes, bbox):\n",
        "    # Read Image\n",
        "    image = load_image(image_path)\n",
        "    bounding_boxes = {\n",
        "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
        "        \"boxes\": bbox,\n",
        "    }\n",
        "    return {\"images\": tf.cast(image, dtype=tf.float32),\n",
        "            \"bounding_boxes\": bounding_boxes}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Augmentation \n",
        "https://keras.io/api/keras_cv/layers/augmentation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr55UhLBwocr"
      },
      "outputs": [],
      "source": [
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.layers.RandomFlip(\n",
        "            mode=\"horizontal\",\n",
        "            bounding_box_format=bounding_box_format),\n",
        "        keras_cv.layers.RandomShear(\n",
        "            x_factor=0.2,\n",
        "            y_factor=0.2,\n",
        "            bounding_box_format=bounding_box_format\n",
        "        ),\n",
        "        keras_cv.layers.JitteredResize(\n",
        "        target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=bounding_box_format\n",
        "    ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mixFYPZ5wocr"
      },
      "outputs": [],
      "source": [
        "resizing = keras_cv.layers.Resizing(\n",
        "    640, 640,\n",
        "    bounding_box_format=bounding_box_format,\n",
        "    pad_to_aspect_ratio=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRBFlQk1wocs"
      },
      "outputs": [],
      "source": [
        "# create training dataset\n",
        "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
        "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1EJxs_ywocs"
      },
      "outputs": [],
      "source": [
        "# create validation dataset\n",
        "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
        "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe1sN85Nwocs"
      },
      "outputs": [],
      "source": [
        "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
        "    inputs = next(iter(inputs.take(1)))\n",
        "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=value_range,\n",
        "        rows=rows,\n",
        "        cols=cols,\n",
        "        y_true=bounding_boxes,\n",
        "        scale=5,\n",
        "        font_scale=0.7,\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        class_mapping=class_mapping,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu-Sv8nSwoct"
      },
      "outputs": [],
      "source": [
        "visualize_dataset(\n",
        "    train_ds, bounding_box_format=bounding_box_format, value_range=(0, 255), rows=2, cols=2\n",
        ")\n",
        "\n",
        "visualize_dataset(\n",
        "    val_ds, bounding_box_format=bounding_box_format, value_range=(0, 255), rows=2, cols=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzlkJrdZwoct"
      },
      "outputs": [],
      "source": [
        "def dict_to_tuple(inputs):\n",
        "    return inputs[\"images\"], bounding_box.to_dense(\n",
        "        inputs[\"bounding_boxes\"], max_boxes=32\n",
        "    )\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psz2nQr7woct"
      },
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcAR-eINwocy"
      },
      "source": [
        "We can switch the backbone by using `keras_cv.models.YOLOV8Detector.from_preset` and another [preset](https://keras.io/api/keras_cv/models/tasks/yolo_v8_detector/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl_oyV-jwocz"
      },
      "outputs": [],
      "source": [
        " # We will use yolov8 small backbone with coco weights\n",
        "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
        "    \"yolo_v8_s_backbone_coco\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Todo: fine tune decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZFgBiSewocz"
      },
      "outputs": [],
      "source": [
        "\n",
        "prediction_decoder = keras_cv.layers.NonMaxSuppression(\n",
        "    bounding_box_format=bounding_box_format,\n",
        "    from_logits=True,\n",
        "    iou_threshold=0.2,\n",
        "    confidence_threshold=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Px1nBO-wocz"
      },
      "source": [
        "The `NonMaxSuppression` layer is responsible to prune underconfident boxes. Raising the `confidence_threshold` will cause the model to only output boxes that have a higher confidence score. `iou_threshold` controls the threshold of intersection over union (IoU) that two boxes must have in order for one to be pruned out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h8G2KhEwoc0"
      },
      "source": [
        "Next, let's build a YOLOV8 model using the `YOLOV8Detector`, which accepts a feature extractor as the `backbone` argument, a `num_classes` argument that specifies the number of object classes to detect based on the size of the `class_mapping` list, a `bounding_box_format` argument that informs the model of the format of the bbox in the dataset, and a finally, the feature pyramid network (FPN) depth is specified by the `fpn_depth` argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlXoP67lwoc0"
      },
      "source": [
        "# Compilation\n",
        "Loss used for YOLOV8\n",
        "\n",
        "1. Classification Loss: This loss function calculates the discrepancy between anticipated class probabilities and actual class probabilities. In this instance, `binary_crossentropy`, a prominent solution for binary classification issues, is Utilized. We Utilized binary crossentropy since each thing that is identified is either classed as belonging to or not belonging to a certain object class (such as a person, a car, etc.).\n",
        "\n",
        "2. Box Loss: `box_loss` is the loss function used to measure the difference between the predicted bounding boxes and the ground truth. In this case, the Complete IoU (CIoU) metric is used, which not only measures the overlap between predicted and ground truth bounding boxes but also considers the difference in aspect ratio, center distance, and box size. Together, these loss functions help optimize the model for object detection by minimizing the difference between the predicted and ground truth class probabilities and bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxFeDpVFwoc0"
      },
      "outputs": [],
      "source": [
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=bounding_box_format,\n",
        "    backbone=backbone,\n",
        "    fpn_depth=1,\n",
        "   # prediction_decoder=prediction_decoder\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhx_UoPAwoc0"
      },
      "source": [
        "You will always want to include a global_clipnorm when training object detection models. This is to remedy exploding gradient problems that frequently occur when training object detection models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzsNH3pPwoc0"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    global_clipnorm=GLOBAL_CLIPNORM,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RFJt9Jnwoc0"
      },
      "source": [
        "To achieve the best results on your dataset, you'll likely want to hand craft a `PiecewiseConstantDecay` learning rate schedule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRv9hvTSwoc0"
      },
      "outputs": [],
      "source": [
        "yolo.compile(\n",
        "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyRNDJYFwoc0"
      },
      "source": [
        "## COCO Metric Callback\n",
        "The most popular object detection metrics are COCO metrics, which were published alongside the MSCOCO dataset. KerasCV provides an easy-to-use suite of COCO metrics under the `keras_cv.callbacks.PyCOCOCallback` symbol. Note that we use a Keras callback instead of a Keras metric to compute COCO metrics. This is because computing COCO metrics requires storing all of a model's predictions for the entire evaluation dataset in memory at once, which is impractical to do during training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJYHTVn4woc1"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CMl98Agwoc1"
      },
      "outputs": [],
      "source": [
        "coco_metrics_callback = keras_cv.callbacks.PyCOCOCallback(\n",
        "    val_ds,\n",
        "    bounding_box_format)\n",
        "\n",
        "yolo.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=2,\n",
        "    callbacks=[coco_metrics_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1OXhn2hMqDi"
      },
      "outputs": [],
      "source": [
        "yolo.save(\"./drive/MyDrive/model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqDka7j3woc1"
      },
      "source": [
        "# Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q33twgxlwoc1"
      },
      "outputs": [],
      "source": [
        "def visualize_detections(model, dataset, bounding_box_format):\n",
        "    images, y_true = next(iter(dataset.take(1)))\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred = bounding_box.to_ragged(y_pred)\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        scale=4,\n",
        "        rows=2,\n",
        "        cols=2,\n",
        "        show=True,\n",
        "        font_scale=0.7,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "\n",
        "visualize_detections(yolo, dataset=val_ds, bounding_box_format=bounding_box_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkc1afXbwoc1"
      },
      "source": [
        "## Function to visualize a prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxXlowv4woc1"
      },
      "outputs": [],
      "source": [
        "y_pred = pretrained_model.predict(image_batch)\n",
        "visualization.plot_bounding_box_gallery(\n",
        "    image_batch,\n",
        "    value_range=(0, 255),\n",
        "    rows=1,\n",
        "    cols=1,\n",
        "    y_pred=y_pred,\n",
        "    scale=5,\n",
        "    font_scale=0.7,\n",
        "    bounding_box_format=bounding_box_format,\n",
        "    class_mapping=class_mapping,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
