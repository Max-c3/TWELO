{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"TensorFlow\"\n",
    "\n",
    "from tqdm.auto import tqdm # show progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_cv\n",
    "\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 5\n",
    "GLOBAL_CLIPNORM = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'car', 1: 'pedestrian', 2: 'trafficLight', 3: 'biker', 4: 'truck'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dictionary for the classes\n",
    "class_ids = [\n",
    "    \"car\",\n",
    "    \"pedestrian\",\n",
    "    \"trafficLight\",\n",
    "    \"biker\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to images and annotations\n",
    "path_images = \"../data/example_data/images\"\n",
    "path_annot = \"../data/example_data/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all XML file paths in path_annot and sort them\n",
    "txt_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_annot, file_name)\n",
    "        for file_name in os.listdir(path_annot)\n",
    "        if file_name.endswith(\".txt\")\n",
    "    ]\n",
    ")\n",
    "txt_files = txt_files[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/example_data/labels/1478019952686311006_jpg.rf.54e2d12dbabc46be3c78995b6eaf3fee.txt'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(txt_file):\n",
    "    with open(txt_file) as file:\n",
    "        lines = file.readlines()\n",
    "        file_name = file.name\n",
    "\n",
    "    image_path = os.path.join(path_images, file_name)\n",
    "    boxes = []\n",
    "    class_ids = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "\n",
    "        cls = int(line[0])\n",
    "        class_ids.append(cls)\n",
    "\n",
    "        xmin = float(line[1])\n",
    "        ymin = float(line[2])\n",
    "        xmax = float(line[3])\n",
    "        ymax = float(line[4])\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "    return image_path, boxes, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "bbox = []\n",
    "classes = []\n",
    "for txt_file in txt_files:\n",
    "    image_path, boxes, class_ids = parse_annotation(txt_file)\n",
    "    image_paths.append(image_path)\n",
    "    bbox.append(boxes)\n",
    "    classes.append(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 1, 1, 2], [10, 1, 1, 2], [10, 1, 10, 1], [10, 1, 10, 1]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.43359375, 0.48828125, 0.0166015625, 0.0283203125],\n",
       "  [0.458984375, 0.494140625, 0.0244140625, 0.0263671875],\n",
       "  [0.5087890625, 0.4970703125, 0.0283203125, 0.0380859375],\n",
       "  [0.9287109375, 0.5107421875, 0.0361328125, 0.21875]],\n",
       " [[0.43359375, 0.48833333333333334, 0.01614583333333333, 0.028333333333333335],\n",
       "  [0.45859374999999997,\n",
       "   0.4941666666666667,\n",
       "   0.024479166666666666,\n",
       "   0.02666666666666667],\n",
       "  [0.5088541666666666, 0.49750000000000005, 0.028125, 0.03833333333333334],\n",
       "  [0.9286458333333333,\n",
       "   0.5108333333333334,\n",
       "   0.036458333333333336,\n",
       "   0.21833333333333335]],\n",
       " [[0.39453125, 0.49609375, 0.017578125, 0.0302734375],\n",
       "  [0.4130859375, 0.5029296875, 0.0224609375, 0.025390625],\n",
       "  [0.8125, 0.46875, 0.1123046875, 0.0849609375],\n",
       "  [0.4677734375, 0.5078125, 0.0283203125, 0.0380859375]],\n",
       " [[0.39479166666666665,\n",
       "   0.49583333333333335,\n",
       "   0.017708333333333333,\n",
       "   0.030000000000000002],\n",
       "  [0.41328125, 0.5029166666666667, 0.022395833333333334, 0.025833333333333337],\n",
       "  [0.8122395833333333, 0.4683333333333334, 0.11197916666666667, 0.085],\n",
       "  [0.46822916666666664, 0.5075000000000001, 0.028125, 0.03833333333333334]]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating ragged tensors because not the number of objects varies\n",
    "# from image to image\n",
    "bbox = tf.ragged.constant(bbox)\n",
    "classes = tf.ragged.constant(classes)\n",
    "image_paths = tf.ragged.constant(image_paths)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "# Determine number of validation data\n",
    "num_val = int(len(txt_files) * SPLIT_RATIO)\n",
    "\n",
    "# split into train and validation\n",
    "# TODO change into random split via train_test_split\n",
    "val_data = data.take(num_val)\n",
    "train_data = data.skip(num_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kestrix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
